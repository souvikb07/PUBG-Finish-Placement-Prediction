{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# The number of entries to read in. Use it to have fast turn-around. The values are separate for train and test sets\nmax_events_trn=None\nmax_events_tst=None\n# Number on CV folds\nn_cv=3",
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b812bf2c5c85b19f30ca849289c5245b098a9ec6"
      },
      "cell_type": "code",
      "source": "import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=Warning)\n\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n\nimport os\nprint(os.listdir(\"../input\"))",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": "['train.csv', 'sample_submission.csv', 'test.csv']\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "288975a0695a9be91b81b34a02a184d90e6363a9"
      },
      "cell_type": "markdown",
      "source": "Define a function to reduce memory foorprint"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3b3fe875129c04abb0a1a538313599eeae2319ae"
      },
      "cell_type": "code",
      "source": "def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object and col_type.name != 'category' and 'datetime' not in col_type.name:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        elif 'datetime' not in col_type.name:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "91d70e9a594198604de51bdbcfae736af7b8679a"
      },
      "cell_type": "markdown",
      "source": "## Read in the data"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e586f44627ccb9f37dd7499093841aa1058f4522"
      },
      "cell_type": "code",
      "source": "df_trn = pd.read_csv('../input/train.csv', nrows=max_events_trn)\ndf_trn = reduce_mem_usage(df_trn)\n\ndf_tst = pd.read_csv('../input/test.csv',  nrows=max_events_tst)\ndf_tst = reduce_mem_usage(df_tst)",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Memory usage of dataframe is 864.34 MB\nMemory usage after optimization is: 178.69 MB\nDecreased by 79.3%\nMemory usage of dataframe is 356.28 MB\nMemory usage after optimization is: 73.04 MB\nDecreased by 79.5%\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4edf21e98d77f762e247eae03dd6fb4e76354b2f"
      },
      "cell_type": "code",
      "source": "# Data \ndf_trn.head()",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 5,
          "data": {
            "text/plain": "   Id  groupId  matchId      ...       weaponsAcquired  winPoints  winPlacePerc\n0   0       24        0      ...                     4       1458      0.856934\n1   1   440875        1      ...                     3       1511      0.040009\n2   2   878242        2      ...                     5       1583      0.740723\n3   3  1319841        3      ...                     1       1489      0.114624\n4   4  1757883        4      ...                     4       1475      0.521484\n\n[5 rows x 26 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>groupId</th>\n      <th>matchId</th>\n      <th>assists</th>\n      <th>boosts</th>\n      <th>damageDealt</th>\n      <th>DBNOs</th>\n      <th>headshotKills</th>\n      <th>heals</th>\n      <th>killPlace</th>\n      <th>killPoints</th>\n      <th>kills</th>\n      <th>killStreaks</th>\n      <th>longestKill</th>\n      <th>maxPlace</th>\n      <th>numGroups</th>\n      <th>revives</th>\n      <th>rideDistance</th>\n      <th>roadKills</th>\n      <th>swimDistance</th>\n      <th>teamKills</th>\n      <th>vehicleDestroys</th>\n      <th>walkDistance</th>\n      <th>weaponsAcquired</th>\n      <th>winPoints</th>\n      <th>winPlacePerc</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>24</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>247.25000</td>\n      <td>2</td>\n      <td>0</td>\n      <td>4</td>\n      <td>17</td>\n      <td>1050</td>\n      <td>2</td>\n      <td>1</td>\n      <td>65.312500</td>\n      <td>29</td>\n      <td>28</td>\n      <td>1</td>\n      <td>591.5</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>782.500000</td>\n      <td>4</td>\n      <td>1458</td>\n      <td>0.856934</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>440875</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>37.65625</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>45</td>\n      <td>1072</td>\n      <td>1</td>\n      <td>1</td>\n      <td>13.546875</td>\n      <td>26</td>\n      <td>23</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>119.625000</td>\n      <td>3</td>\n      <td>1511</td>\n      <td>0.040009</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>878242</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>93.75000</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n      <td>54</td>\n      <td>1404</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>28</td>\n      <td>28</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3248.000000</td>\n      <td>5</td>\n      <td>1583</td>\n      <td>0.740723</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>1319841</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>95.87500</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>86</td>\n      <td>1069</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>97</td>\n      <td>94</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>21.484375</td>\n      <td>1</td>\n      <td>1489</td>\n      <td>0.114624</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>1757883</td>\n      <td>4</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.00000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>58</td>\n      <td>1034</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>47</td>\n      <td>41</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>641.000000</td>\n      <td>4</td>\n      <td>1475</td>\n      <td>0.521484</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "14da025b31ba03eb18e287e75e2218caa3270df7",
        "trusted": true
      },
      "cell_type": "code",
      "source": "df_trn.info(memory_usage='deep', verbose=False)",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 4357336 entries, 0 to 4357335\nColumns: 26 entries, Id to winPlacePerc\ndtypes: float16(6), int16(2), int32(3), int8(15)\nmemory usage: 178.7 MB\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4090c8229eadbdc5bd299bbc7e30707b39908f11"
      },
      "cell_type": "code",
      "source": "df_tst.info(memory_usage='deep', verbose=False)",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1867913 entries, 0 to 1867912\nColumns: 25 entries, Id to winPoints\ndtypes: float16(5), int16(2), int32(3), int8(15)\nmemory usage: 73.0 MB\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8912101f11d42d319c5ee8bf5e7d890741b4da96"
      },
      "cell_type": "markdown",
      "source": "- The training dataset has 4.3M entries, which is not small and aloows for advanced models like GBM and NN to dominate.\n- The test dataset is only 1.9M entries\n- There are 25 features (+ the target in the train dataset)"
    },
    {
      "metadata": {
        "_uuid": "9a0c8d0216165720b7d7d320670999a66df5903c"
      },
      "cell_type": "markdown",
      "source": "## Missing data"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4a8ef393a096c219ac78f0e02ba0938d1ab9b13f"
      },
      "cell_type": "code",
      "source": "df_trn.isnull().sum()",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 8,
          "data": {
            "text/plain": "Id                 0\ngroupId            0\nmatchId            0\nassists            0\nboosts             0\ndamageDealt        0\nDBNOs              0\nheadshotKills      0\nheals              0\nkillPlace          0\nkillPoints         0\nkills              0\nkillStreaks        0\nlongestKill        0\nmaxPlace           0\nnumGroups          0\nrevives            0\nrideDistance       0\nroadKills          0\nswimDistance       0\nteamKills          0\nvehicleDestroys    0\nwalkDistance       0\nweaponsAcquired    0\nwinPoints          0\nwinPlacePerc       0\ndtype: int64"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "collapsed": true,
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": false
      },
      "cell_type": "markdown",
      "source": "No missing data"
    },
    {
      "metadata": {
        "_uuid": "5da0af5d18692cade6ad13cc75eeff3933e13635"
      },
      "cell_type": "markdown",
      "source": "## Prepare the data"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bba8e2eb85a46ad85a3a047b3a895746ea4da414"
      },
      "cell_type": "code",
      "source": "y = df_trn['winPlacePerc']\ndf_trn.drop('winPlacePerc', axis=1, inplace=True)",
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7a1d0f60824b4140cd071a8ad3c0cbca9756adf4"
      },
      "cell_type": "markdown",
      "source": "We will NOT use Id, groupId, matchId. The first one is a unique identifier and can be useful only in the case of data leakage. The other two would be useful in feature engineering with grouped stats per match and per team."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "229d3a13f58a95fb31699c9ec7ada89f0c939ad9"
      },
      "cell_type": "code",
      "source": "# we will NOT use \nfeatures_not2use = ['Id', 'groupId', 'matchId']",
      "execution_count": 11,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ee45078488b4160e8c0daca52c8f51c936ff624b"
      },
      "cell_type": "code",
      "source": "for df in [df_trn, df_tst]:\n    df.drop(features_not2use, axis=1, inplace=True)",
      "execution_count": 12,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2b64053f97bc5190d1951c2cbb21ee5800366f22"
      },
      "cell_type": "markdown",
      "source": "## Train and evaluate a model\nStart by defining handy helper functions..."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9156d4c66c30d4238af04fe2ec62d2c1a6a13595"
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.base import clone, ClassifierMixin, RegressorMixin\nimport lightgbm as lgb\n\n\ndef learning_rate_decay_power(current_iter):\n    '''\n    The function defines learning rate deay for LGBM\n    '''\n    base_learning_rate = 0.10\n    min_lr = 5e-2\n    lr = base_learning_rate  * np.power(.996, current_iter)\n    return lr if lr > min_lr else min_lr\n\n\ndef train_single_model(clf_, X_, y_, random_state_=314, opt_parameters_={}, fit_params_={}):\n    '''\n    A wrapper to train a model with particular parameters\n    '''\n    c = clone(clf_)\n    c.set_params(**opt_parameters_)\n    c.set_params(random_state=random_state_)\n    return c.fit(X_, y_, **fit_params_)\n\ndef train_model_in_CV(model, X, y, metric, metric_args={},\n                            model_name='xmodel',\n                            seed=31416, n=5,\n                            opt_parameters_={}, fit_params_={},\n                            verbose=True):\n    # the list of classifiers for voting ensable\n    clfs = []\n    # performance \n    perf_eval = {'score_i_oof': 0,\n                 'score_i_ave': 0,\n                 'score_i_std': 0,\n                 'score_i': []\n                }\n    # full-sample oof prediction\n    y_full_oof = pd.Series(np.zeros(shape=(y.shape[0],)), \n                          index=y.index)\n    \n    if 'sample_weight' in metric_args:\n        sample_weight=metric_args['sample_weight']\n        \n    doSqrt=False\n    if 'sqrt' in metric_args:\n        doSqrt=True\n        del metric_args['sqrt']\n\n    cv = KFold(n, shuffle=True, random_state=seed) #Stratified\n    # The out-of-fold (oof) prediction for the k-1 sample in the outer CV loop\n    y_oof = pd.Series(np.zeros(shape=(X.shape[0],)), \n                      index=X.index)\n    scores = []\n    clfs = []\n\n    for n_fold, (trn_idx, val_idx) in enumerate(cv.split(X, (y!=0).astype(np.int8))):\n        X_trn, y_trn = X.iloc[trn_idx], y.iloc[trn_idx]\n        X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n\n        if fit_params_:\n            # use _stp data for early stopping\n            fit_params_[\"eval_set\"] = [(X_trn,y_trn), (X_val,y_val)]\n            fit_params_['verbose'] = verbose\n\n        clf = train_single_model(model, X_trn, y_trn, 314+n_fold, opt_parameters_, fit_params_)\n\n        clfs.append(('{}{}'.format(model_name,n_fold), clf))\n        # evaluate performance\n        if isinstance(clf, RegressorMixin):\n            y_oof.iloc[val_idx] = clf.predict(X_val)\n        elif isinstance(clf, ClassifierMixin):\n            y_oof.iloc[val_idx] = clf.predict_proba(X_val)[:,1]\n        else:\n            raise TypeError('Provided model does not inherit neither from a regressor nor from classifier')\n        if 'sample_weight' in metric_args:\n            metric_args['sample_weight'] = y_val.map(sample_weight)\n        scores.append(metric(y_val, y_oof.iloc[val_idx], **metric_args))\n        #cleanup\n        del X_trn, y_trn, X_val, y_val\n\n    # Store performance info for this CV\n    if 'sample_weight' in metric_args:\n        metric_args['sample_weight'] = y_oof.map(sample_weight)\n    perf_eval['score_i_oof'] = metric(y, y_oof, **metric_args)\n    perf_eval['score_i'] = scores\n    \n    if doSqrt:\n        for k in perf_eval.keys():\n            if 'score' in k:\n                perf_eval[k] = np.sqrt(perf_eval[k])\n        scores = np.sqrt(scores)\n            \n    perf_eval['score_i_ave'] = np.mean(scores)\n    perf_eval['score_i_std'] = np.std(scores)\n\n    return clfs, perf_eval, y_oof\n\ndef print_perf_clf(name, perf_eval):\n    print('Performance of the model:')    \n    print('Mean(Val) score inner {} Classifier: {:.4f}+-{:.4f}'.format(name, \n                                                                      perf_eval['score_i_ave'],\n                                                                      perf_eval['score_i_std']\n                                                                     ))\n    print('Min/max scores on folds: {:.4f} / {:.4f}'.format(np.min(perf_eval['score_i']),\n                                                            np.max(perf_eval['score_i'])))\n    print('OOF score inner {} Classifier: {:.4f}'.format(name, perf_eval['score_i_oof']))\n    print('Scores in individual folds: {}'.format(perf_eval['score_i']))",
      "execution_count": 13,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "19870fc24b6181c5f9cd929d1d872d6427f4dcd3"
      },
      "cell_type": "markdown",
      "source": "Now let's define the parameter and model in a scalable fashion (we can add later on further models to the list and it will work out-of-the-box).\n\nThe format is a dictionary with keys that are user model names and items being an array (or tuple) of:\n\n- model to be fitted;\n- additional model parameters to be set;\n- model fit parameters (they are passed to model.fit() call);\n- target variable."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2017daa8c393a396b4b4eb02f0f4b6909e2454b3"
      },
      "cell_type": "code",
      "source": "mdl_inputs = {\n        # This will be with MAE loss\n        'lgbm1_reg': (lgb.LGBMRegressor(max_depth=-1, min_child_samples=400, random_state=314, silent=True, metric='None', n_jobs=4, n_estimators=5000, learning_rate=0.05),\n                 {'objective': 'mae', 'colsample_bytree': 0.75, 'min_child_weight': 10.0, 'num_leaves': 30, 'reg_alpha': 1, 'subsample': 0.75}, \n                 {\"early_stopping_rounds\":100, \n                  \"eval_metric\" : 'mae',\n                  'eval_names': ['train', 'early_stop'],\n                  'verbose': False, \n                  'callbacks': [lgb.reset_parameter(learning_rate=learning_rate_decay_power)],\n                  'categorical_feature': 'auto'},\n                 y\n                ),\n        # This will be with FAIR loss\n        'lgbm2_reg': (lgb.LGBMRegressor(max_depth=-1, min_child_samples=400, random_state=314, silent=True, metric='None', n_jobs=4, n_estimators=5000, learning_rate=0.05),\n                 {'objective': 'fair', 'colsample_bytree': 0.75, 'min_child_weight': 10.0, 'num_leaves': 30, 'reg_alpha': 1, 'subsample': 0.75}, \n                 {\"early_stopping_rounds\":100, \n                  \"eval_metric\" : 'mae',\n                  'eval_names': ['train', 'early_stop'],\n                  'verbose': False, \n                  'callbacks': [lgb.reset_parameter(learning_rate=learning_rate_decay_power)],\n                  'categorical_feature': 'auto'},\n                 y\n                ),\n       }",
      "execution_count": 14,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "26ee0ffb4e7106bd0ea1a78ccd19b206240b78ac"
      },
      "cell_type": "markdown",
      "source": "Do the actual model training"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d9708e73085acf783fcdcce7bcbb1b300a1fff40"
      },
      "cell_type": "code",
      "source": "%%time\nmdls = {}\nresults = {}\ny_oofs = {}\nfor name, (mdl, mdl_pars, fit_pars, y_) in mdl_inputs.items():\n    print('--------------- {} -----------'.format(name))\n    mdl_, perf_eval_, y_oof_ = train_model_in_CV(mdl, df_trn, y_, mean_absolute_error, \n                                                          metric_args={},\n                                                          model_name=name, \n                                                          opt_parameters_=mdl_pars,\n                                                          fit_params_=fit_pars, \n                                                          n=n_cv,\n                                                          verbose=False)\n    results[name] = perf_eval_\n    mdls[name] = mdl_\n    y_oofs[name] = y_oof_\n    print_perf_clf(name, perf_eval_)",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": "--------------- lgbm1_reg -----------\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f99db069d8b2fd05e7e81997b3f55c57bd972c0d"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}